{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b4f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f3d410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/5_rcdjz121v7rnhqrmsz6df80000gn/T/ipykernel_1886/1706210101.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('/Users/dennis_m_jose/Documents/GitHub/Santander-RecSys-Dennis/data/cleaned_santander_data.csv')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('/Users/dennis_m_jose/Documents/GitHub/Santander-RecSys-Dennis/data/cleaned_santander_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c473c7",
   "metadata": {},
   "source": [
    "## BASELINE MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6e931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1375586</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050611</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050612</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050613</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>1050614</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687899</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1297410</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687900</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1297411</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687901</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1297412</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687902</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1297392</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687903</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1297395</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12687904 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fecha_dato  ncodpers ind_empleado pais_residencia sexo   age  \\\n",
       "0         2015-01-28   1375586            N              ES    H  35.0   \n",
       "1         2015-01-28   1050611            N              ES    V  23.0   \n",
       "2         2015-01-28   1050612            N              ES    V  23.0   \n",
       "3         2015-01-28   1050613            N              ES    H  22.0   \n",
       "4         2015-01-28   1050614            N              ES    V  23.0   \n",
       "...              ...       ...          ...             ...  ...   ...   \n",
       "12687899  2016-04-28   1297410            N              ES    V  47.0   \n",
       "12687900  2016-04-28   1297411            N              ES    H  41.0   \n",
       "12687901  2016-04-28   1297412            N              ES    V  34.0   \n",
       "12687902  2016-04-28   1297392            N              ES    H  44.0   \n",
       "12687903  2016-04-28   1297395            N              ES    V  29.0   \n",
       "\n",
       "          fecha_alta  ind_nuevo  antiguedad  indrel  ... ind_hip_fin_ult1  \\\n",
       "0         2015-01-12        0.0           6     1.0  ...                0   \n",
       "1         2012-08-10        0.0          35     1.0  ...                0   \n",
       "2         2012-08-10        0.0          35     1.0  ...                0   \n",
       "3         2012-08-10        0.0          35     1.0  ...                0   \n",
       "4         2012-08-10        0.0          35     1.0  ...                0   \n",
       "...              ...        ...         ...     ...  ...              ...   \n",
       "12687899  2014-08-18        0.0          20     1.0  ...                0   \n",
       "12687900  2014-08-18        0.0          20     1.0  ...                0   \n",
       "12687901  2014-08-18        0.0          20     1.0  ...                0   \n",
       "12687902  2014-08-18        0.0          20     1.0  ...                0   \n",
       "12687903  2014-08-18        0.0          20     1.0  ...                0   \n",
       "\n",
       "         ind_plan_fin_ult1 ind_pres_fin_ult1 ind_reca_fin_ult1  \\\n",
       "0                        0                 0                 0   \n",
       "1                        0                 0                 0   \n",
       "2                        0                 0                 0   \n",
       "3                        0                 0                 0   \n",
       "4                        0                 0                 0   \n",
       "...                    ...               ...               ...   \n",
       "12687899                 0                 0                 0   \n",
       "12687900                 0                 0                 0   \n",
       "12687901                 0                 0                 0   \n",
       "12687902                 0                 0                 0   \n",
       "12687903                 0                 0                 0   \n",
       "\n",
       "         ind_tjcr_fin_ult1 ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "0                        0                 0                 0   \n",
       "1                        0                 0                 0   \n",
       "2                        0                 0                 0   \n",
       "3                        0                 0                 0   \n",
       "4                        0                 0                 0   \n",
       "...                    ...               ...               ...   \n",
       "12687899                 0                 0                 0   \n",
       "12687900                 0                 0                 0   \n",
       "12687901                 0                 0                 0   \n",
       "12687902                 0                 0                 0   \n",
       "12687903                 0                 0                 0   \n",
       "\n",
       "          ind_nomina_ult1 ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "0                     0.0               0.0                0  \n",
       "1                     0.0               0.0                0  \n",
       "2                     0.0               0.0                0  \n",
       "3                     0.0               0.0                0  \n",
       "4                     0.0               0.0                0  \n",
       "...                   ...               ...              ...  \n",
       "12687899              0.0               0.0                0  \n",
       "12687900              0.0               0.0                0  \n",
       "12687901              0.0               0.0                1  \n",
       "12687902              0.0               0.0                0  \n",
       "12687903              0.0               0.0                0  \n",
       "\n",
       "[12687904 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['fecha_dato'] < '2016-05-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44132b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016-05-28'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['fecha_dato'] >= '2016-05-01']['fecha_dato'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9564270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split: train on data before May 2016, validate on May 2016\n",
    "train_df = df_train[df_train['fecha_dato'] < '2016-05-01']\n",
    "eval_df = df_train[df_train['fecha_dato'] == '2016-05-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c8089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cols = [col for col in df_train.columns if col.endswith('ult1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871d0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_cco_fin_ult1', 'ind_ctop_fin_ult1', 'ind_recibo_ult1', 'ind_ecue_fin_ult1', 'ind_cno_fin_ult1', 'ind_nom_pens_ult1', 'ind_nomina_ult1']\n"
     ]
    }
   ],
   "source": [
    "# Sum of additions per product\n",
    "product_popularity = train_df[product_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Get top 7 products\n",
    "top_7_products = product_popularity.head(7).index.tolist()\n",
    "print(top_7_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bef4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique customers in May 2016\n",
    "eval_users = eval_df['ncodpers'].unique()\n",
    "\n",
    "# Predict same top 7 for all\n",
    "baseline_preds = pd.DataFrame({\n",
    "    'ncodpers': eval_users,\n",
    "    'added_products': [' '.join(top_7_products)] * len(eval_users)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3280bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    if not actual:\n",
    "        return 0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual_list, predicted_list, k=7):\n",
    "    return np.mean([apk(a, p.split(), k) for a, p in zip(actual_list, predicted_list)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53816027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAP@7: 0.5937\n"
     ]
    }
   ],
   "source": [
    "# Ground truth from eval_df\n",
    "eval_truth = []\n",
    "for _, row in eval_df.groupby('ncodpers'):\n",
    "    added = [prod for prod in product_cols if row[prod].values[0] == 1]\n",
    "    eval_truth.append(added)\n",
    "\n",
    "# Compute MAP@7\n",
    "map_score = mapk(eval_truth, baseline_preds['added_products'].tolist())\n",
    "print(f\"Baseline MAP@7: {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "716439f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>added_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>657640</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>657788</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>657795</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>657790</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>657794</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931448</th>\n",
       "      <td>1166765</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931449</th>\n",
       "      <td>1166764</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931450</th>\n",
       "      <td>1166763</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931451</th>\n",
       "      <td>1166789</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931452</th>\n",
       "      <td>1550586</td>\n",
       "      <td>ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>931453 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ncodpers                                     added_products\n",
       "0         657640  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "1         657788  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "2         657795  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "3         657790  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "4         657794  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "...          ...                                                ...\n",
       "931448   1166765  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "931449   1166764  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "931450   1166763  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "931451   1166789  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "931452   1550586  ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_...\n",
       "\n",
       "[931453 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075061f",
   "metadata": {},
   "source": [
    "## Baseline Model Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions into list of products\n",
    "baseline_preds['prediction_list'] = baseline_preds['added_products'].apply(lambda x: x.split())\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    \"ncodpers\": baseline_preds['ncodpers'],\n",
    "    \"actual\": eval_truth,\n",
    "    \"predicted\": baseline_preds['prediction_list']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9871e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  False Positives & False Negatives\n",
    "comparison['false_positives'] = comparison.apply(\n",
    "    lambda row: [p for p in row['predicted'] if p not in row['actual']], axis=1)\n",
    "\n",
    "comparison['false_negatives'] = comparison.apply(\n",
    "    lambda row: [a for a in row['actual'] if a not in row['predicted']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffef22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top False Positives (Over-predicted products):\n",
      "ind_nomina_ult1      883144\n",
      "ind_nom_pens_ult1    878605\n",
      "ind_cno_fin_ult1     858392\n",
      "ind_ecue_fin_ult1    855608\n",
      "ind_ctop_fin_ult1    830864\n",
      "ind_recibo_ult1      818403\n",
      "ind_cco_fin_ult1     369838\n",
      "dtype: int64\n",
      "\n",
      "Top False Negatives (Missed products):\n",
      "ind_reca_fin_ult1    45535\n",
      "ind_tjcr_fin_ult1    34822\n",
      "ind_ctpp_fin_ult1    33271\n",
      "ind_dela_fin_ult1    31217\n",
      "ind_valo_fin_ult1    21426\n",
      "ind_fond_fin_ult1    14687\n",
      "ind_ctma_fin_ult1     8097\n",
      "ind_ctju_fin_ult1     7581\n",
      "ind_plan_fin_ult1     7359\n",
      "ind_hip_fin_ult1      4528\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count most common FP/FN products\n",
    "fp_counts = pd.Series([p for row in comparison['false_positives'] for p in row]).value_counts()\n",
    "fn_counts = pd.Series([p for row in comparison['false_negatives'] for p in row]).value_counts()\n",
    "\n",
    "print(\"\\nTop False Positives (Over-predicted products):\")\n",
    "print(fp_counts.head(10))\n",
    "\n",
    "print(\"\\nTop False Negatives (Missed products):\")\n",
    "print(fn_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ad4e7",
   "metadata": {},
   "source": [
    "## Item based Collaborative Filtering\n",
    "\n",
    "This model finds similarities between products based on co-occurrence patterns - if many users who have Product A also have Product B, they are considered similar.\n",
    "\n",
    "Algorithm:\n",
    "1. Build user-product interaction matrix (users × products)\n",
    "2. Calculate cosine similarity between all product pairs\n",
    "3. For each user:\n",
    "   - Find products they currently own\n",
    "   - Sum similarity scores from owned products to all other products\n",
    "   - Recommend top 7 products with highest aggregate similarity\n",
    "\n",
    "#### Similarity between products i and j:\n",
    "- similarity(i,j) = cos(θ) = (A·B)/(||A|| × ||B||)\n",
    "\n",
    "#### Recommendation score for product p:\n",
    "- score(p) = Σ similarity(owned_product, p) for all owned products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e6a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item similarity matrix shape: (24, 24)\n",
      "Sparsity: 0.0646\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create user-item interaction matrix from training data\n",
    "def create_interaction_matrix(train_df, product_cols):\n",
    "    \"\"\"\n",
    "    Create sparse matrix of user-product interactions\n",
    "    \"\"\"\n",
    "    # Get unique users\n",
    "    users = train_df['ncodpers'].unique()\n",
    "    user_to_idx = {user: idx for idx, user in enumerate(users)}\n",
    "    \n",
    "    # Create product to index mapping\n",
    "    product_to_idx = {prod: idx for idx, prod in enumerate(product_cols)}\n",
    "    \n",
    "    # Aggregate interactions per user (taking max to handle multiple months)\n",
    "    user_products = train_df.groupby('ncodpers')[product_cols].max()\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data = []\n",
    "    \n",
    "    for user in user_products.index:\n",
    "        if user in user_to_idx:\n",
    "            user_idx = user_to_idx[user]\n",
    "            for prod in product_cols:\n",
    "                if user_products.loc[user, prod] == 1:\n",
    "                    row_indices.append(user_idx)\n",
    "                    col_indices.append(product_to_idx[prod])\n",
    "                    data.append(1)\n",
    "    \n",
    "    interaction_matrix = csr_matrix(\n",
    "        (data, (row_indices, col_indices)), \n",
    "        shape=(len(users), len(product_cols))\n",
    "    )\n",
    "    \n",
    "    return interaction_matrix, user_to_idx, product_to_idx\n",
    "\n",
    "# Build item similarity matrix\n",
    "interaction_matrix, user_to_idx, product_to_idx = create_interaction_matrix(train_df, product_cols)\n",
    "\n",
    "# Compute item-item similarity (transpose for item-based)\n",
    "item_matrix = interaction_matrix.T\n",
    "item_similarity = cosine_similarity(item_matrix, dense_output=False)\n",
    "\n",
    "# Convert to dense for easier manipulation (if memory allows)\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity.toarray(),\n",
    "    index=product_cols,\n",
    "    columns=product_cols\n",
    ")\n",
    "\n",
    "print(f\"Item similarity matrix shape: {item_similarity_df.shape}\")\n",
    "print(f\"Sparsity: {(interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1])):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ea02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item-Based CF MAP@7: 0.0898\n",
      "Baseline MAP@7: 0.5937\n",
      "Improvement: -84.87%\n"
     ]
    }
   ],
   "source": [
    "def get_item_based_recommendations(user_history, item_similarity_df, n_recs=7, exclude_owned=True):\n",
    "    \"\"\"\n",
    "    Generate recommendations based on items the user already has\n",
    "    \"\"\"\n",
    "    # If user has no history, return empty list\n",
    "    if not user_history:\n",
    "        return []\n",
    "    \n",
    "    # Calculate scores for all products\n",
    "    scores = {}\n",
    "    \n",
    "    for candidate_item in item_similarity_df.columns:\n",
    "        # Skip if user already owns this product\n",
    "        if exclude_owned and candidate_item in user_history:\n",
    "            continue\n",
    "            \n",
    "        # Sum similarities from all owned items\n",
    "        score = 0\n",
    "        for owned_item in user_history:\n",
    "            if owned_item in item_similarity_df.index:\n",
    "                score += item_similarity_df.loc[owned_item, candidate_item]\n",
    "        \n",
    "        scores[candidate_item] = score\n",
    "    \n",
    "    # Sort and get top N\n",
    "    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [item for item, score in sorted_items[:n_recs]]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate predictions for evaluation set\n",
    "cf_predictions = []\n",
    "\n",
    "for user_id in eval_users:\n",
    "    # Get user's current products (from last available month in training)\n",
    "    user_train_data = train_df[train_df['ncodpers'] == user_id]\n",
    "    \n",
    "    if len(user_train_data) > 0:\n",
    "        # Get most recent month's products\n",
    "        latest_products = user_train_data.iloc[-1][product_cols]\n",
    "        owned_products = [prod for prod in product_cols if latest_products[prod] == 1]\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recs = get_item_based_recommendations(\n",
    "            owned_products, \n",
    "            item_similarity_df, \n",
    "            n_recs=7\n",
    "        )\n",
    "    else:\n",
    "        # Cold start: fallback to popular items\n",
    "        recs = top_7_products\n",
    "    \n",
    "    cf_predictions.append(recs)\n",
    "\n",
    "# Create predictions dataframe\n",
    "cf_preds_df = pd.DataFrame({\n",
    "    'ncodpers': eval_users,\n",
    "    'cf_predictions': cf_predictions,\n",
    "    'cf_predictions_str': [' '.join(preds) for preds in cf_predictions]\n",
    "})\n",
    "\n",
    "# Calculate MAP@7 for CF\n",
    "cf_map_score = mapk(eval_truth, cf_preds_df['cf_predictions_str'].tolist())\n",
    "print(f\"\\nItem-Based CF MAP@7: {cf_map_score:.4f}\")\n",
    "print(f\"Baseline MAP@7: {map_score:.4f}\")\n",
    "print(f\"Improvement: {((cf_map_score - map_score) / map_score * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebd1e8",
   "metadata": {},
   "source": [
    "### Evaluation on test data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8897934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def generate_cf_predictions_optimized(test_users, train_df, product_cols,\n",
    "                                      item_similarity_df, top_7_products):\n",
    "    \"\"\"\n",
    "    Ultra-fast CF prediction using matrix operations\n",
    "    \"\"\"\n",
    "    print(\"Preparing data structures...\")\n",
    "    \n",
    "    # Get April 2016 data (last known state before May predictions)\n",
    "    april_data = train_df[train_df['fecha_dato'] == '2016-04-28']\n",
    "    april_data_indexed = april_data.set_index('ncodpers')\n",
    "    april_users_set = set(april_data_indexed.index)\n",
    "    \n",
    "    # Create mappings\n",
    "    product_to_idx = {prod: idx for idx, prod in enumerate(product_cols)}\n",
    "    idx_to_product = {idx: prod for idx, prod in enumerate(product_cols)}\n",
    "    \n",
    "    # Pre-convert similarity matrix to numpy for faster access\n",
    "    sim_matrix = item_similarity_df.values\n",
    "    \n",
    "    # Prepare baseline for cold-start users\n",
    "    baseline_pred = ' '.join(top_7_products)\n",
    "    \n",
    "    print(f\"Processing {len(test_users)} users...\")\n",
    "    print(f\"Users with history: {len(april_users_set & set(test_users))}\")\n",
    "    \n",
    "    # Process all users with vectorized operations where possible\n",
    "    predictions = []\n",
    "    \n",
    "    # Group users by whether they have history\n",
    "    users_with_history = []\n",
    "    users_without_history = []\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        if user_id in april_users_set:\n",
    "            users_with_history.append(user_id)\n",
    "        else:\n",
    "            users_without_history.append(user_id)\n",
    "    \n",
    "    print(f\"Users with history: {len(users_with_history)}\")\n",
    "    print(f\"Users without history: {len(users_without_history)}\")\n",
    "    \n",
    "    # Process users with history in batches\n",
    "    batch_size = 5000\n",
    "    user_pred_map = {}\n",
    "    \n",
    "    for i in range(0, len(users_with_history), batch_size):\n",
    "        batch_users = users_with_history[i:i+batch_size]\n",
    "        \n",
    "        # Get product ownership for batch\n",
    "        batch_data = april_data_indexed.loc[batch_users][product_cols].values\n",
    "        \n",
    "        # Compute recommendations using matrix multiplication\n",
    "        # Shape: (batch_size, n_products) @ (n_products, n_products) = (batch_size, n_products)\n",
    "        recommendation_scores = batch_data @ sim_matrix\n",
    "        \n",
    "        # Mask out already owned products\n",
    "        recommendation_scores[batch_data == 1] = -np.inf\n",
    "        \n",
    "        # Generate predictions for batch\n",
    "        for j, user_id in enumerate(batch_users):\n",
    "            user_scores = recommendation_scores[j]\n",
    "            \n",
    "            # Get top 7 products\n",
    "            top_indices = np.argsort(user_scores)[-7:][::-1]\n",
    "            \n",
    "            # Filter valid recommendations (score > 0)\n",
    "            valid_recs = [idx_to_product[idx] for idx in top_indices \n",
    "                         if user_scores[idx] > 0]\n",
    "            \n",
    "            if len(valid_recs) >= 7:\n",
    "                pred = ' '.join(valid_recs[:7])\n",
    "            elif len(valid_recs) > 0:\n",
    "                # Pad with popular products not already recommended\n",
    "                remaining = [p for p in top_7_products if p not in valid_recs]\n",
    "                pred = ' '.join(valid_recs + remaining[:7-len(valid_recs)])\n",
    "            else:\n",
    "                pred = baseline_pred\n",
    "            \n",
    "            user_pred_map[user_id] = pred\n",
    "        \n",
    "        if (i + batch_size) % 50000 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(users_with_history))}/{len(users_with_history)} users with history\")\n",
    "    \n",
    "    # Assign baseline to users without history\n",
    "    for user_id in users_without_history:\n",
    "        user_pred_map[user_id] = baseline_pred\n",
    "    \n",
    "    # Create final predictions list in original order\n",
    "    predictions = [user_pred_map[user_id] for user_id in test_users]\n",
    "    \n",
    "    print(\"Prediction generation complete!\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc26d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/5_rcdjz121v7rnhqrmsz6df80000gn/T/ipykernel_1886/3810902245.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv('/Users/dennis_m_jose/Documents/GitHub/Santander-RecSys-Dennis/data/test_ver2.csv')  # Update path as needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test users: 929615\n",
      "Preparing data structures...\n",
      "Processing 929615 users...\n",
      "Users with history: 925252\n",
      "Users with history: 925252\n",
      "Users without history: 4363\n",
      "Processed 50000/925252 users with history\n",
      "Processed 100000/925252 users with history\n",
      "Processed 150000/925252 users with history\n",
      "Processed 200000/925252 users with history\n",
      "Processed 250000/925252 users with history\n",
      "Processed 300000/925252 users with history\n",
      "Processed 350000/925252 users with history\n",
      "Processed 400000/925252 users with history\n",
      "Processed 450000/925252 users with history\n",
      "Processed 500000/925252 users with history\n",
      "Processed 550000/925252 users with history\n",
      "Processed 600000/925252 users with history\n",
      "Processed 650000/925252 users with history\n",
      "Processed 700000/925252 users with history\n",
      "Processed 750000/925252 users with history\n",
      "Processed 800000/925252 users with history\n",
      "Processed 850000/925252 users with history\n",
      "Processed 900000/925252 users with history\n",
      "Prediction generation complete!\n",
      "Submission saved to cf_submission_batch.csv\n",
      "Shape: (929615, 2)\n",
      "Unique products recommended: 21\n",
      "\n",
      "First 3 predictions:\n",
      "User 15889: ind_recibo_ult1 ind_tjcr_fin_ult1 ind_cno_fin_ult1 ind_ecue_fin_ult1 ind_ctop_fin_ult1 ind_nom_pens_ult1 ind_reca_fin_ult1\n",
      "User 1170544: ind_recibo_ult1 ind_ctop_fin_ult1 ind_ecue_fin_ult1 ind_dela_fin_ult1 ind_tjcr_fin_ult1 ind_cno_fin_ult1 ind_reca_fin_ult1\n",
      "User 1170545: ind_recibo_ult1 ind_ctop_fin_ult1 ind_ecue_fin_ult1 ind_dela_fin_ult1 ind_tjcr_fin_ult1 ind_cno_fin_ult1 ind_reca_fin_ult1\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('/Users/dennis_m_jose/Documents/GitHub/Santander-RecSys-Dennis/data/test_ver2.csv')  # Update path as needed\n",
    "test_users = test_df['ncodpers'].values\n",
    "\n",
    "print(f\"Total test users: {len(test_users)}\")\n",
    "\n",
    "# Generate CF predictions using batch processing\n",
    "cf_predictions_batch = generate_cf_predictions_optimized(\n",
    "    test_users, \n",
    "    df_train,  # Your training data\n",
    "    product_cols,  # Your product columns\n",
    "    item_similarity_df,  # Your similarity matrix\n",
    "    top_7_products  # Your top 7 popular products\n",
    ")\n",
    "\n",
    "# Create submission file\n",
    "cf_submission_batch = pd.DataFrame({\n",
    "    'ncodpers': test_users,\n",
    "    'added_products': cf_predictions_batch\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "cf_submission_batch.to_csv('cf_submission_batch.csv', index=False)\n",
    "\n",
    "print(\"Submission saved to cf_submission_batch.csv\")\n",
    "print(f\"Shape: {cf_submission_batch.shape}\")\n",
    "\n",
    "# Quick validation\n",
    "unique_products = set(' '.join(cf_predictions_batch).split())\n",
    "print(f\"Unique products recommended: {len(unique_products)}\")\n",
    "print(\"\\nFirst 3 predictions:\")\n",
    "for i in range(3):\n",
    "    print(f\"User {test_users[i]}: {cf_predictions_batch[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03526ce6",
   "metadata": {},
   "source": [
    "#### Key Characteristics\n",
    "\n",
    "- Personalization: Recommendations based on user's current product portfolio\n",
    "- Sparsity: 6.46% density in interaction matrix\n",
    "- Performance: MAP@7 = 0.0898 (local), 0.014 (Kaggle)\n",
    "\n",
    "#### Why It Underperformed\n",
    "\n",
    "Sparse similarity matrix: Many products lack sufficient co-occurrence data\n",
    "Temporal aggregation issue: Using .max() across all months may include future information\n",
    "Poor cold start handling: Users with few/no products get weak recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb141fc",
   "metadata": {},
   "source": [
    "## Hybrid Approach (Item based CF + Popularity)\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "The hybrid model combines popularity-based and collaborative filtering scores using a weighted average, controlled by parameter α (alpha).\n",
    "\n",
    "Algorithm:\n",
    "1. Calculate popularity score for each product (normalized 0-1)\n",
    "2. Calculate CF similarity scores for each product (normalized 0-1)\n",
    "3. Combine scores: hybrid_score = α × CF_score + (1-α) × popularity_score\n",
    "4. Mask out products user already owns\n",
    "5. Recommend top 7 products with highest hybrid scores\n",
    "\n",
    "#### For each product p and user u:\n",
    "- popularity_score(p) = count(p) / max_count\n",
    "\n",
    "- cf_score(u,p) = Σ similarity(owned, p) / max_similarity\n",
    "\n",
    "- hybrid_score(u,p) = α × cf_score(u,p) + (1-α) × popularity_score(p)\n",
    "\n",
    "#####\n",
    "\n",
    "- #### Where α ∈ [0,1]:\n",
    "- #### α = 0: Pure popularity (no personalization)\n",
    "- #### α = 0.3: 30% CF, 70% popularity (balanced)\n",
    "- #### α = 1: Pure CF (maximum personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc44f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hybrid_predictions_fast(users, train_df, product_cols, \n",
    "                                     item_similarity_df, top_7_products,\n",
    "                                     alpha=0.3, reference_date='2016-04-28'):\n",
    "    \"\"\"\n",
    "    Fast hybrid model using vectorized operations\n",
    "    Alpha: weight for CF (0.3 = 30% CF, 70% popularity)\n",
    "    reference_date: date to use for user's last known products\n",
    "    \"\"\"\n",
    "    print(f\"Generating hybrid predictions with alpha={alpha} (CF: {alpha:.0%}, Popularity: {(1-alpha):.0%})\")\n",
    "    \n",
    "    # Pre-compute popularity scores\n",
    "    product_popularity = train_df[product_cols].sum()\n",
    "    pop_scores = product_popularity / product_popularity.max()\n",
    "    pop_array = pop_scores.values  # Convert to numpy for speed\n",
    "    \n",
    "    # Get reference data (April for test, or last month for validation)\n",
    "    reference_data = train_df[train_df['fecha_dato'] == reference_date]\n",
    "    reference_data_indexed = reference_data.set_index('ncodpers')\n",
    "    reference_users_set = set(reference_data_indexed.index)\n",
    "    \n",
    "    # Pre-compute similarity matrix as numpy\n",
    "    sim_matrix = item_similarity_df.values\n",
    "    \n",
    "    # Baseline prediction\n",
    "    baseline_pred = ' '.join(top_7_products)\n",
    "    \n",
    "    # Process users in batches\n",
    "    predictions = []\n",
    "    batch_size = 10000\n",
    "    users_with_history = [u for u in users if u in reference_users_set]\n",
    "    users_without_history = [u for u in users if u not in reference_users_set]\n",
    "    \n",
    "    print(f\"Users with history: {len(users_with_history)}, without: {len(users_without_history)}\")\n",
    "    \n",
    "    # Create user prediction map\n",
    "    user_pred_map = {}\n",
    "    \n",
    "    # Process users with history in batches\n",
    "    for i in range(0, len(users_with_history), batch_size):\n",
    "        batch_users = users_with_history[i:i+batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_data = reference_data_indexed.loc[batch_users][product_cols].values\n",
    "        \n",
    "        # Compute CF scores via matrix multiplication\n",
    "        cf_scores_batch = batch_data @ sim_matrix\n",
    "        \n",
    "        # Normalize CF scores per user\n",
    "        cf_max = np.maximum(cf_scores_batch.max(axis=1, keepdims=True), 1e-10)\n",
    "        cf_scores_normalized = cf_scores_batch / cf_max\n",
    "        \n",
    "        # Combine with popularity scores\n",
    "        hybrid_scores = alpha * cf_scores_normalized + (1 - alpha) * pop_array\n",
    "        \n",
    "        # Mask owned products\n",
    "        hybrid_scores[batch_data == 1] = -np.inf\n",
    "        \n",
    "        # Get top 7 for each user\n",
    "        for j, user_id in enumerate(batch_users):\n",
    "            top_indices = np.argsort(hybrid_scores[j])[-7:][::-1]\n",
    "            top_products = [product_cols[idx] for idx in top_indices]\n",
    "            user_pred_map[user_id] = ' '.join(top_products)\n",
    "        \n",
    "        if (i + batch_size) % 100000 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(users_with_history))}/{len(users_with_history)}\")\n",
    "    \n",
    "    # Assign baseline to users without history\n",
    "    for user_id in users_without_history:\n",
    "        user_pred_map[user_id] = baseline_pred\n",
    "    \n",
    "    # Create final predictions in original order\n",
    "    predictions = [user_pred_map[user_id] for user_id in users]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d00a4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_alpha_values(test_users_sample, train_df, product_cols, \n",
    "                      item_similarity_df, top_7_products):\n",
    "    \"\"\"\n",
    "    Quick test of different alpha values on a sample\n",
    "    \"\"\"\n",
    "    sample_size = min(10000, len(test_users_sample))\n",
    "    sample_users = test_users_sample[:sample_size]\n",
    "    \n",
    "    alphas = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    results = {}\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nTesting alpha={alpha}\")\n",
    "        preds = generate_hybrid_predictions_fast(\n",
    "            sample_users, train_df, product_cols,\n",
    "            item_similarity_df, top_7_products, alpha\n",
    "        )\n",
    "        \n",
    "        # Calculate diversity\n",
    "        unique_products = len(set(' '.join(preds).split()))\n",
    "        results[alpha] = unique_products\n",
    "        print(f\"  Unique products: {unique_products}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b5cd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_eval_set(train_df, eval_df, product_cols, item_similarity_df, \n",
    "                         top_7_products, alphas=[0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "    \"\"\"\n",
    "    Test different alpha values on the evaluation set\n",
    "    \"\"\"\n",
    "    # Get eval users and ground truth\n",
    "    eval_users = eval_df['ncodpers'].unique()\n",
    "    \n",
    "    # Build ground truth\n",
    "    eval_truth = []\n",
    "    for _, row in eval_df.groupby('ncodpers'):\n",
    "        added = [prod for prod in product_cols if row[prod].values[0] == 1]\n",
    "        eval_truth.append(added)\n",
    "    \n",
    "    # For validation, we need to use April data to predict May\n",
    "    # But train only on data before May\n",
    "    train_before_may = train_df[train_df['fecha_dato'] < '2016-05-01']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing alpha={alpha}\")\n",
    "        \n",
    "        # Generate predictions with this alpha\n",
    "        predictions = generate_hybrid_predictions_fast(\n",
    "            eval_users, \n",
    "            train_before_may,\n",
    "            product_cols,\n",
    "            item_similarity_df,\n",
    "            top_7_products,\n",
    "            alpha=alpha,\n",
    "            reference_date='2016-04-28'  # Use April to predict May\n",
    "        )\n",
    "        \n",
    "        # Calculate MAP@7\n",
    "        from collections import namedtuple\n",
    "        \n",
    "        def apk(actual, predicted, k=7):\n",
    "            if len(predicted) > k:\n",
    "                predicted = predicted[:k]\n",
    "            if not actual:\n",
    "                return 0\n",
    "            score = 0.0\n",
    "            num_hits = 0.0\n",
    "            for i, p in enumerate(predicted):\n",
    "                if p in actual and p not in predicted[:i]:\n",
    "                    num_hits += 1.0\n",
    "                    score += num_hits / (i+1.0)\n",
    "            return score / min(len(actual), k)\n",
    "        \n",
    "        def mapk(actual_list, predicted_list, k=7):\n",
    "            return np.mean([apk(a, p.split(), k) for a, p in zip(actual_list, predicted_list)])\n",
    "        \n",
    "        map_score = mapk(eval_truth, predictions)\n",
    "        \n",
    "        # Calculate diversity\n",
    "        unique_products = len(set(' '.join(predictions).split()))\n",
    "        \n",
    "        results[alpha] = {\n",
    "            'MAP@7': map_score,\n",
    "            'unique_products': unique_products\n",
    "        }\n",
    "        \n",
    "        print(f\"  MAP@7: {map_score:.4f}\")\n",
    "        print(f\"  Unique products: {unique_products}\")\n",
    "        \n",
    "        # Compare with baseline\n",
    "        if 'map_score' in locals() or 'map_score' in globals():\n",
    "            baseline_map = 0.5937  # Your baseline score\n",
    "            improvement = ((map_score - baseline_map) / baseline_map) * 100\n",
    "            print(f\"  vs Baseline: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Find best alpha\n",
    "    best_alpha = max(results, key=lambda x: results[x]['MAP@7'])\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BEST ALPHA: {best_alpha} with MAP@7: {results[best_alpha]['MAP@7']:.4f}\")\n",
    "    \n",
    "    return results, best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd94ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating hybrid model on eval set...\n",
      "\n",
      "==================================================\n",
      "Testing alpha=0.1\n",
      "Generating hybrid predictions with alpha=0.1 (CF: 10%, Popularity: 90%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.2928\n",
      "  Unique products: 20\n",
      "  vs Baseline: -50.68%\n",
      "\n",
      "==================================================\n",
      "Testing alpha=0.2\n",
      "Generating hybrid predictions with alpha=0.2 (CF: 20%, Popularity: 80%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.2908\n",
      "  Unique products: 21\n",
      "  vs Baseline: -51.02%\n",
      "\n",
      "==================================================\n",
      "Testing alpha=0.3\n",
      "Generating hybrid predictions with alpha=0.3 (CF: 30%, Popularity: 70%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.2889\n",
      "  Unique products: 21\n",
      "  vs Baseline: -51.34%\n",
      "\n",
      "==================================================\n",
      "Testing alpha=0.4\n",
      "Generating hybrid predictions with alpha=0.4 (CF: 40%, Popularity: 60%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.2868\n",
      "  Unique products: 21\n",
      "  vs Baseline: -51.70%\n",
      "\n",
      "==================================================\n",
      "Testing alpha=0.5\n",
      "Generating hybrid predictions with alpha=0.5 (CF: 50%, Popularity: 50%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.2857\n",
      "  Unique products: 21\n",
      "  vs Baseline: -51.88%\n",
      "\n",
      "==================================================\n",
      "BEST ALPHA: 0.1 with MAP@7: 0.2928\n",
      "\n",
      "Summary of results:\n",
      "Alpha 0.1: MAP@7=0.2928, Products=20\n",
      "Alpha 0.2: MAP@7=0.2908, Products=21\n",
      "Alpha 0.3: MAP@7=0.2889, Products=21\n",
      "Alpha 0.4: MAP@7=0.2868, Products=21\n",
      "Alpha 0.5: MAP@7=0.2857, Products=21\n"
     ]
    }
   ],
   "source": [
    "# Validate on eval set to find best alpha\n",
    "print(\"Validating hybrid model on eval set...\")\n",
    "results, best_alpha = validate_on_eval_set(\n",
    "    train_df, \n",
    "    eval_df, \n",
    "    product_cols, \n",
    "    item_similarity_df, \n",
    "    top_7_products,\n",
    "    alphas=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    ")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "for alpha, metrics in results.items():\n",
    "    print(f\"Alpha {alpha}: MAP@7={metrics['MAP@7']:.4f}, Products={metrics['unique_products']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b15a4e",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "\n",
    "All hybrid configurations underperform: Every alpha value produces MAP@7 around 0.29, which is ~50% worse than your baseline (0.5937)\n",
    "\n",
    "Inverse relationship: As CF weight increases (higher alpha), performance decreases\n",
    "\n",
    "This confirms CF is actively hurting predictions\n",
    "\n",
    "\n",
    "Limited product diversity: Only 20-21 unique products being recommended across all users\n",
    "\n",
    "##### What's Going Wrong? \n",
    "\n",
    "The discrepancy between the baseline score (0.5937) and hybrid scores (~0.29) suggests a fundamental implementation issue:\n",
    "\n",
    "##### Validation vs Test Mismatch:\n",
    "\n",
    "- Your baseline MAP@7 of 0.5937 seems unusually high for local validation\n",
    "- Kaggle baseline was 0.017, which is more realistic\n",
    "- This suggests the validation setup might be different from how you calculated the baseline\n",
    "\n",
    "\n",
    "##### Possible Data Leakage:\n",
    "\n",
    "The baseline might have been calculated differently (possibly using future information)\n",
    "Or the eval_truth calculation might be different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f5c1a",
   "metadata": {},
   "source": [
    "#### Error Anlaysis or Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fafacd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval users count: 931453\n",
      "Baseline predictions count: 931453\n",
      "Generating hybrid predictions with alpha=0.1 (CF: 10%, Popularity: 90%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "\n",
      "User 657640:\n",
      "  Baseline pred: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "  Hybrid pred: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "  Actual products: ['ind_cco_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1']\n",
      "\n",
      "User 657788:\n",
      "  Baseline pred: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "  Hybrid pred: ind_recibo_ult1 ind_ctop_fin_ult1 ind_ecue_fin_ult...\n",
      "  Actual products: ['ind_cno_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_ecue_fin_ult1', 'ind_plan_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
      "\n",
      "User 657795:\n",
      "  Baseline pred: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "  Hybrid pred: ind_recibo_ult1 ind_ctop_fin_ult1 ind_ecue_fin_ult...\n",
      "  Actual products: ['ind_cco_fin_ult1', 'ind_dela_fin_ult1', 'ind_ecue_fin_ult1', 'ind_reca_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_recibo_ult1']\n",
      "\n",
      "Recalculated baseline MAP@7: 0.5937\n",
      "Generating hybrid predictions with alpha=0.0 (CF: 0%, Popularity: 100%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "Pure popularity (alpha=0) MAP@7: 0.2964\n",
      "Should match baseline: 0.5937\n",
      "\n",
      "Predictions matching between baseline and alpha=0: 265150/931453\n"
     ]
    }
   ],
   "source": [
    "# 1. Verify baseline calculation on same eval set\n",
    "print(f\"Eval users count: {len(eval_users)}\")\n",
    "print(f\"Baseline predictions count: {len(baseline_preds)}\")\n",
    "\n",
    "# 2. Check a few predictions manually\n",
    "# First, let's get the predictions from the hybrid model\n",
    "# We need to run the hybrid model again to store predictions\n",
    "hybrid_test_predictions = generate_hybrid_predictions_fast(\n",
    "    eval_users, \n",
    "    train_df,\n",
    "    product_cols,\n",
    "    item_similarity_df,\n",
    "    top_7_products,\n",
    "    alpha=0.1,\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    user = eval_users[i]\n",
    "    print(f\"\\nUser {user}:\")\n",
    "    print(f\"  Baseline pred: {baseline_preds[baseline_preds['ncodpers']==user]['added_products'].values[0][:50]}...\")\n",
    "    print(f\"  Hybrid pred: {hybrid_test_predictions[i][:50]}...\")\n",
    "    print(f\"  Actual products: {eval_truth[i]}\")\n",
    "\n",
    "# 3. Recalculate baseline MAP@7 using same function\n",
    "baseline_map_recalc = mapk(eval_truth, baseline_preds['added_products'].tolist())\n",
    "print(f\"\\nRecalculated baseline MAP@7: {baseline_map_recalc:.4f}\")\n",
    "\n",
    "# 4. Also test pure popularity (alpha=0) which should match baseline\n",
    "pure_popularity_predictions = generate_hybrid_predictions_fast(\n",
    "    eval_users, \n",
    "    train_df,\n",
    "    product_cols,\n",
    "    item_similarity_df,\n",
    "    top_7_products,\n",
    "    alpha=0.0,  # Pure popularity\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "pure_pop_map = mapk(eval_truth, pure_popularity_predictions)\n",
    "print(f\"Pure popularity (alpha=0) MAP@7: {pure_pop_map:.4f}\")\n",
    "print(f\"Should match baseline: {baseline_map_recalc:.4f}\")\n",
    "\n",
    "# 5. Check if predictions are identical\n",
    "matches = sum(1 for i in range(len(eval_users)) \n",
    "              if baseline_preds['added_products'].iloc[i] == pure_popularity_predictions[i])\n",
    "print(f\"\\nPredictions matching between baseline and alpha=0: {matches}/{len(eval_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1695c",
   "metadata": {},
   "source": [
    "##### Critical Issue Found: The Hybrid Model Implementation Has a Bug\n",
    "\n",
    "The output reveals the problem clearly:\n",
    "\n",
    "Key Discovery\n",
    "\n",
    "- Baseline MAP@7: 0.5937 (confirmed)\n",
    "- Pure popularity (alpha=0) MAP@7: 0.2964 (should be identical to baseline)\n",
    "- Matching predictions: Only 265,150/931,453 (28.5%) match between baseline and alpha=0\n",
    "\n",
    "This proves there's a bug in the hybrid model implementation, not a conceptual issue.\n",
    "\n",
    "**The Problem**\n",
    "\n",
    "When alpha=0 (pure popularity), the hybrid model should produce exactly the same predictions as your baseline, but it doesn't. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333a01e",
   "metadata": {},
   "source": [
    "The hybrid model is excluding products the user already owns, but your baseline doesn't do this! Your evaluation is likely measuring which products users have in May that they didn't have in April (new acquisitions), but the hybrid model is excluding all products they already own.\n",
    "\n",
    "The Fix\n",
    "\n",
    "We need to modify the hybrid model to NOT exclude owned products when making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a1049ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hybrid predictions with alpha=0.0 (CF: 0%, Popularity: 100%)\n",
      "Fixed model with alpha=0 MAP@7: 0.5937\n",
      "Should now match baseline: 0.5937\n"
     ]
    }
   ],
   "source": [
    "def generate_hybrid_predictions_fixed(users, train_df, product_cols, \n",
    "                                      item_similarity_df, top_7_products,\n",
    "                                      alpha=0.3, reference_date='2016-04-28'):\n",
    "    \"\"\"\n",
    "    Fixed hybrid model - doesn't exclude owned products\n",
    "    \"\"\"\n",
    "    print(f\"Generating hybrid predictions with alpha={alpha} (CF: {alpha:.0%}, Popularity: {(1-alpha):.0%})\")\n",
    "    \n",
    "    # Pre-compute popularity scores\n",
    "    product_popularity = train_df[product_cols].sum()\n",
    "    pop_scores = product_popularity / product_popularity.max()\n",
    "    pop_array = pop_scores.values\n",
    "    \n",
    "    # Get reference data\n",
    "    reference_data = train_df[train_df['fecha_dato'] == reference_date]\n",
    "    reference_data_indexed = reference_data.set_index('ncodpers')\n",
    "    reference_users_set = set(reference_data_indexed.index)\n",
    "    \n",
    "    # Pre-compute similarity matrix\n",
    "    sim_matrix = item_similarity_df.values\n",
    "    \n",
    "    # Baseline prediction\n",
    "    baseline_pred = ' '.join(top_7_products)\n",
    "    \n",
    "    # If alpha is 0, just return popularity for everyone\n",
    "    if alpha == 0:\n",
    "        return [baseline_pred] * len(users)\n",
    "    \n",
    "    # Process users\n",
    "    predictions = []\n",
    "    batch_size = 10000\n",
    "    users_with_history = [u for u in users if u in reference_users_set]\n",
    "    users_without_history = [u for u in users if u not in reference_users_set]\n",
    "    \n",
    "    print(f\"Users with history: {len(users_with_history)}, without: {len(users_without_history)}\")\n",
    "    \n",
    "    user_pred_map = {}\n",
    "    \n",
    "    # Process users with history\n",
    "    for i in range(0, len(users_with_history), batch_size):\n",
    "        batch_users = users_with_history[i:i+batch_size]\n",
    "        batch_data = reference_data_indexed.loc[batch_users][product_cols].values\n",
    "        \n",
    "        # Compute CF scores\n",
    "        cf_scores_batch = batch_data @ sim_matrix\n",
    "        cf_max = np.maximum(cf_scores_batch.max(axis=1, keepdims=True), 1e-10)\n",
    "        cf_scores_normalized = cf_scores_batch / cf_max\n",
    "        \n",
    "        # Combine with popularity\n",
    "        hybrid_scores = alpha * cf_scores_normalized + (1 - alpha) * pop_array\n",
    "        \n",
    "        # DON'T mask owned products - we want to recommend all top products\n",
    "        # hybrid_scores[batch_data == 1] = -np.inf  # REMOVED THIS LINE\n",
    "        \n",
    "        # Get top 7 for each user\n",
    "        for j, user_id in enumerate(batch_users):\n",
    "            top_indices = np.argsort(hybrid_scores[j])[-7:][::-1]\n",
    "            top_products = [product_cols[idx] for idx in top_indices]\n",
    "            user_pred_map[user_id] = ' '.join(top_products)\n",
    "        \n",
    "        if (i + batch_size) % 100000 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(users_with_history))}/{len(users_with_history)}\")\n",
    "    \n",
    "    # Users without history get baseline\n",
    "    for user_id in users_without_history:\n",
    "        user_pred_map[user_id] = baseline_pred\n",
    "    \n",
    "    # Create final predictions\n",
    "    predictions = [user_pred_map[user_id] for user_id in users]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test the fixed version\n",
    "fixed_predictions = generate_hybrid_predictions_fixed(\n",
    "    eval_users, \n",
    "    train_df,\n",
    "    product_cols,\n",
    "    item_similarity_df,\n",
    "    top_7_products,\n",
    "    alpha=0.0  # Pure popularity\n",
    ")\n",
    "\n",
    "fixed_map = mapk(eval_truth, fixed_predictions)\n",
    "print(f\"Fixed model with alpha=0 MAP@7: {fixed_map:.4f}\")\n",
    "print(f\"Should now match baseline: {baseline_map_recalc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e090f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing alpha=0.0\n",
      "Generating hybrid predictions with alpha=0.0 (CF: 0%, Popularity: 100%)\n",
      "  MAP@7: 0.5937\n",
      "  Unique products: 7\n",
      "  vs Baseline: -0.00%\n",
      "\n",
      "Testing alpha=0.05\n",
      "Generating hybrid predictions with alpha=0.05 (CF: 5%, Popularity: 95%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.5916\n",
      "  Unique products: 12\n",
      "  vs Baseline: -0.35%\n",
      "\n",
      "Testing alpha=0.1\n",
      "Generating hybrid predictions with alpha=0.1 (CF: 10%, Popularity: 90%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.5908\n",
      "  Unique products: 23\n",
      "  vs Baseline: -0.49%\n",
      "\n",
      "Testing alpha=0.15\n",
      "Generating hybrid predictions with alpha=0.15 (CF: 15%, Popularity: 85%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.5897\n",
      "  Unique products: 23\n",
      "  vs Baseline: -0.67%\n",
      "\n",
      "Testing alpha=0.2\n",
      "Generating hybrid predictions with alpha=0.2 (CF: 20%, Popularity: 80%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.5870\n",
      "  Unique products: 23\n",
      "  vs Baseline: -1.13%\n",
      "\n",
      "Testing alpha=0.3\n",
      "Generating hybrid predictions with alpha=0.3 (CF: 30%, Popularity: 70%)\n",
      "Users with history: 926663, without: 4790\n",
      "Processed 100000/926663\n",
      "Processed 200000/926663\n",
      "Processed 300000/926663\n",
      "Processed 400000/926663\n",
      "Processed 500000/926663\n",
      "Processed 600000/926663\n",
      "Processed 700000/926663\n",
      "Processed 800000/926663\n",
      "Processed 900000/926663\n",
      "  MAP@7: 0.5851\n",
      "  Unique products: 23\n",
      "  vs Baseline: -1.45%\n",
      "\n",
      "==================================================\n",
      "BEST ALPHA: 0.0 with MAP@7: 0.5937\n",
      "\n",
      "Summary of Fixed Model Results:\n",
      "Alpha 0.00: MAP@7=0.5937 (-0.00%), Products=7\n",
      "Alpha 0.05: MAP@7=0.5916 (-0.35%), Products=12\n",
      "Alpha 0.10: MAP@7=0.5908 (-0.49%), Products=23\n",
      "Alpha 0.15: MAP@7=0.5897 (-0.67%), Products=23\n",
      "Alpha 0.20: MAP@7=0.5870 (-1.13%), Products=23\n",
      "Alpha 0.30: MAP@7=0.5851 (-1.45%), Products=23\n"
     ]
    }
   ],
   "source": [
    "# Test different alpha values with the FIXED model\n",
    "def validate_fixed_hybrid(train_df, eval_df, product_cols, item_similarity_df, \n",
    "                         top_7_products, eval_users, eval_truth,\n",
    "                         alphas=[0.0, 0.05, 0.1, 0.15, 0.2, 0.3]):\n",
    "    \"\"\"\n",
    "    Test the fixed hybrid model with different alpha values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nTesting alpha={alpha}\")\n",
    "        \n",
    "        predictions = generate_hybrid_predictions_fixed(\n",
    "            eval_users, \n",
    "            train_df,\n",
    "            product_cols,\n",
    "            item_similarity_df,\n",
    "            top_7_products,\n",
    "            alpha=alpha,\n",
    "            reference_date='2016-04-28'\n",
    "        )\n",
    "        \n",
    "        map_score = mapk(eval_truth, predictions)\n",
    "        unique_products = len(set(' '.join(predictions).split()))\n",
    "        \n",
    "        results[alpha] = {\n",
    "            'MAP@7': map_score,\n",
    "            'unique_products': unique_products\n",
    "        }\n",
    "        \n",
    "        print(f\"  MAP@7: {map_score:.4f}\")\n",
    "        print(f\"  Unique products: {unique_products}\")\n",
    "        \n",
    "        # Show improvement/degradation from baseline\n",
    "        baseline_map = 0.5937\n",
    "        change = ((map_score - baseline_map) / baseline_map) * 100\n",
    "        print(f\"  vs Baseline: {change:+.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run validation\n",
    "results_fixed = validate_fixed_hybrid(\n",
    "    train_df, eval_df, product_cols, item_similarity_df, \n",
    "    top_7_products, eval_users, eval_truth,\n",
    "    alphas=[0.0, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    ")\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha_fixed = max(results_fixed, key=lambda x: results_fixed[x]['MAP@7'])\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BEST ALPHA: {best_alpha_fixed} with MAP@7: {results_fixed[best_alpha_fixed]['MAP@7']:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of Fixed Model Results:\")\n",
    "for alpha, metrics in results_fixed.items():\n",
    "    improvement = ((metrics['MAP@7'] - 0.5937) / 0.5937) * 100\n",
    "    print(f\"Alpha {alpha:4.2f}: MAP@7={metrics['MAP@7']:.4f} ({improvement:+.2f}%), Products={metrics['unique_products']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e6825ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating test predictions with alpha=0.05\n",
      "Generating hybrid predictions with alpha=0.05 (CF: 5%, Popularity: 95%)\n",
      "Users with history: 925252, without: 4363\n",
      "Processed 100000/925252\n",
      "Processed 200000/925252\n",
      "Processed 300000/925252\n",
      "Processed 400000/925252\n",
      "Processed 500000/925252\n",
      "Processed 600000/925252\n",
      "Processed 700000/925252\n",
      "Processed 800000/925252\n",
      "Processed 900000/925252\n",
      "Saved to hybrid_alpha005.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate test predictions with alpha=0.05\n",
    "chosen_alpha = 0.05  # Minimal CF influence\n",
    "\n",
    "print(f\"\\nGenerating test predictions with alpha={chosen_alpha}\")\n",
    "test_predictions = generate_hybrid_predictions_fixed(\n",
    "    test_users,\n",
    "    df_train,\n",
    "    product_cols,\n",
    "    item_similarity_df,\n",
    "    top_7_products,\n",
    "    alpha=chosen_alpha,\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ncodpers': test_users,\n",
    "    'added_products': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('hybrid_alpha005.csv', index=False)\n",
    "print(\"Saved to hybrid_alpha005.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b6a95",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "### What is Matrix Factorization?\n",
    "\n",
    "Matrix Factorization is like finding the \"hidden reasons\" why users choose certain products. Instead of looking at individual product similarities, it discovers underlying patterns.\n",
    "\n",
    "Analogy: Imagine Netflix movies. Instead of comparing movies directly, Matrix Factorization might discover \n",
    "\n",
    "hidden factors like:\n",
    "\n",
    "- Factor 1: \"Action vs Romance\"\n",
    "- Factor 2: \"Indie vs Blockbuster\"\n",
    "- Factor 3: \"Comedy vs Drama\"\n",
    "\n",
    "For banking, these factors might represent things like:\n",
    "\n",
    "- Factor 1: \"Basic vs Premium products\"\n",
    "- Factor 2: \"Savings vs Credit products\"\n",
    "- Factor 3: \"Digital vs Traditional banking\n",
    "\n",
    "### How The Algorithm Works\n",
    "\n",
    "MATRIX FACTORIZATION ALGORITHM:\n",
    "\n",
    "1. START with User-Product Matrix (931K users × 24 products)\n",
    "   - 1 = user has product\n",
    "   - 0 = user doesn't have product\n",
    "\n",
    "2. DECOMPOSE into smaller matrices:\n",
    "   Original Matrix ≈ User_Factors × Product_Factors\n",
    "   (931K × 24)   ≈  (931K × 15)  ×  (15 × 24)\n",
    "   \n",
    "3. DISCOVER latent features:\n",
    "   - Each user gets 15 numbers (their preferences for each hidden factor)\n",
    "   - Each product gets 15 numbers (how much it relates to each factor)\n",
    "\n",
    "4. PREDICT scores:\n",
    "   Score(user, product) = user_factors · product_factors\n",
    "   \n",
    "5. RECOMMEND top 7 products with highest scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acad62",
   "metadata": {},
   "source": [
    "### Building Matrix\n",
    "\n",
    "*create_matrix_factorization_model(train_df, product_cols, method='svd', n_factors=15)*\n",
    "\n",
    "What it does:\n",
    "\n",
    "- Takes all user-product interactions\n",
    "- Creates a sparse matrix (mostly zeros)\n",
    "- Uses SVD (Singular Value Decomposition) to find 15 hidden factors\n",
    "- Returns two smaller matrices: user preferences and product characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85bb7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def create_matrix_factorization_model(train_df, product_cols, method='svd', n_factors=10):\n",
    "    \"\"\"\n",
    "    Create matrix factorization model using SVD or NMF\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method: 'svd' or 'nmf'\n",
    "    n_factors: number of latent factors\n",
    "    \"\"\"\n",
    "    print(f\"Building {method.upper()} matrix factorization model with {n_factors} factors...\")\n",
    "    \n",
    "    # Create user-product matrix\n",
    "    users = train_df['ncodpers'].unique()\n",
    "    user_to_idx = {user: idx for idx, user in enumerate(users)}\n",
    "    product_to_idx = {prod: idx for idx, prod in enumerate(product_cols)}\n",
    "    \n",
    "    # Aggregate by taking max (user ever had product)\n",
    "    user_products = train_df.groupby('ncodpers')[product_cols].max()\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    n_users = len(users)\n",
    "    n_products = len(product_cols)\n",
    "    \n",
    "    print(f\"Matrix dimensions: {n_users} users x {n_products} products\")\n",
    "    \n",
    "    # Build sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data = []\n",
    "    \n",
    "    for user in user_products.index:\n",
    "        if user in user_to_idx:\n",
    "            user_idx = user_to_idx[user]\n",
    "            for prod in product_cols:\n",
    "                if user_products.loc[user, prod] == 1:\n",
    "                    row_indices.append(user_idx)\n",
    "                    col_indices.append(product_to_idx[prod])\n",
    "                    data.append(1)\n",
    "    \n",
    "    sparse_matrix = csr_matrix((data, (row_indices, col_indices)), \n",
    "                               shape=(n_users, n_products))\n",
    "    \n",
    "    print(f\"Sparsity: {len(data) / (n_users * n_products):.4f}\")\n",
    "    \n",
    "    if method == 'svd':\n",
    "        # Use Truncated SVD for sparse matrices\n",
    "        model = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "        user_factors = model.fit_transform(sparse_matrix)\n",
    "        product_factors = model.components_.T\n",
    "        \n",
    "    elif method == 'nmf':\n",
    "        # Non-negative Matrix Factorization\n",
    "        model = NMF(n_components=n_factors, init='random', random_state=42, max_iter=200)\n",
    "        user_factors = model.fit_transform(sparse_matrix)\n",
    "        product_factors = model.components_.T\n",
    "    \n",
    "    else:\n",
    "        # Traditional SVD (slower, for smaller matrices)\n",
    "        U, sigma, Vt = svds(sparse_matrix.asfptype(), k=n_factors)\n",
    "        user_factors = U * sigma\n",
    "        product_factors = Vt.T\n",
    "    \n",
    "    print(f\"User factors shape: {user_factors.shape}\")\n",
    "    print(f\"Product factors shape: {product_factors.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'user_factors': user_factors,\n",
    "        'product_factors': product_factors,\n",
    "        'user_to_idx': user_to_idx,\n",
    "        'product_to_idx': product_to_idx,\n",
    "        'idx_to_product': {v: k for k, v in product_to_idx.items()},\n",
    "        'users': users,\n",
    "        'model': model if method in ['svd', 'nmf'] else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a5335",
   "metadata": {},
   "source": [
    "### Generate Predictions\n",
    "\n",
    "*generate_mf_predictions(users, train_df, product_cols, mf_model, top_7_products)*\n",
    "\n",
    "What it does:\n",
    "\n",
    "- For each user: multiplies their 15 factor scores with all products' 15 factors\n",
    "- This gives a score for every product\n",
    "- Sorts and picks top 7 highest scoring products\n",
    "- Users not in training get popular products as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0373d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mf_predictions(users_to_predict, train_df, product_cols, \n",
    "                           mf_model, top_7_products, reference_date='2016-04-28'):\n",
    "    \"\"\"\n",
    "    Generate predictions using matrix factorization\n",
    "    \"\"\"\n",
    "    print(\"Generating MF predictions...\")\n",
    "    \n",
    "    # Get reference data\n",
    "    reference_data = train_df[train_df['fecha_dato'] == reference_date]\n",
    "    reference_users = set(reference_data['ncodpers'].unique())\n",
    "    \n",
    "    # Extract model components\n",
    "    user_factors = mf_model['user_factors']\n",
    "    product_factors = mf_model['product_factors']\n",
    "    user_to_idx = mf_model['user_to_idx']\n",
    "    idx_to_product = mf_model['idx_to_product']\n",
    "    \n",
    "    predictions = []\n",
    "    baseline_pred = ' '.join(top_7_products)\n",
    "    \n",
    "    users_with_factors = 0\n",
    "    users_without_factors = 0\n",
    "    \n",
    "    for user in users_to_predict:\n",
    "        if user in user_to_idx and user in reference_users:\n",
    "            users_with_factors += 1\n",
    "            \n",
    "            # Get user's latent factors\n",
    "            user_idx = user_to_idx[user]\n",
    "            user_vec = user_factors[user_idx]\n",
    "            \n",
    "            # Compute scores for all products\n",
    "            scores = user_vec @ product_factors.T\n",
    "            \n",
    "            # Get user's current products to exclude (optional)\n",
    "            user_data = reference_data[reference_data['ncodpers'] == user]\n",
    "            if len(user_data) > 0:\n",
    "                owned_products = user_data.iloc[0][product_cols].values\n",
    "                # Don't exclude owned products based on previous findings\n",
    "                # scores[owned_products == 1] = -np.inf\n",
    "            \n",
    "            # Get top 7\n",
    "            top_indices = np.argsort(scores)[-7:][::-1]\n",
    "            top_products = [idx_to_product[idx] for idx in top_indices]\n",
    "            pred = ' '.join(top_products)\n",
    "            \n",
    "        else:\n",
    "            users_without_factors += 1\n",
    "            pred = baseline_pred\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        \n",
    "        if len(predictions) % 100000 == 0:\n",
    "            print(f\"Processed {len(predictions)}/{len(users_to_predict)} users\")\n",
    "    \n",
    "    print(f\"Users with factors: {users_with_factors}\")\n",
    "    print(f\"Users without factors (using baseline): {users_without_factors}\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e33d2",
   "metadata": {},
   "source": [
    "### Hybrid Approach on Matrix Factorization\n",
    "\n",
    "*hybrid_mf_popularity(users, train_df, product_cols, mf_model, top_7_products, alpha=0.3)*\n",
    "\n",
    "What it does:\n",
    "\n",
    "- Combines MF scores with popularity scores\n",
    "- Formula: final_score = α × MF_score + (1-α) × popularity_score\n",
    "- α=0.3 means 30% personalization, 70% popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "398a2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_mf_popularity(users_to_predict, train_df, product_cols,\n",
    "                        mf_model, top_7_products, alpha=0.3, reference_date='2016-04-28'):\n",
    "    \"\"\"\n",
    "    Hybrid: Matrix Factorization + Popularity\n",
    "    \"\"\"\n",
    "    print(f\"Generating hybrid MF predictions (alpha={alpha}: {alpha:.0%} MF, {(1-alpha):.0%} popularity)\")\n",
    "    \n",
    "    # Popularity scores\n",
    "    product_popularity = train_df[product_cols].sum()\n",
    "    pop_scores = product_popularity / product_popularity.max()\n",
    "    pop_array = pop_scores.values\n",
    "    \n",
    "    # Reference data\n",
    "    reference_data = train_df[train_df['fecha_dato'] == reference_date]\n",
    "    reference_users = set(reference_data['ncodpers'].unique())\n",
    "    \n",
    "    # Model components\n",
    "    user_factors = mf_model['user_factors']\n",
    "    product_factors = mf_model['product_factors']\n",
    "    user_to_idx = mf_model['user_to_idx']\n",
    "    idx_to_product = mf_model['idx_to_product']\n",
    "    \n",
    "    predictions = []\n",
    "    baseline_pred = ' '.join(top_7_products)\n",
    "    \n",
    "    for user in users_to_predict:\n",
    "        if user in user_to_idx and user in reference_users:\n",
    "            # Get MF scores\n",
    "            user_idx = user_to_idx[user]\n",
    "            user_vec = user_factors[user_idx]\n",
    "            mf_scores = user_vec @ product_factors.T\n",
    "            \n",
    "            # Normalize MF scores\n",
    "            if mf_scores.max() > mf_scores.min():\n",
    "                mf_scores_norm = (mf_scores - mf_scores.min()) / (mf_scores.max() - mf_scores.min())\n",
    "            else:\n",
    "                mf_scores_norm = mf_scores\n",
    "            \n",
    "            # Combine with popularity\n",
    "            hybrid_scores = alpha * mf_scores_norm + (1 - alpha) * pop_array\n",
    "            \n",
    "            # Get top 7\n",
    "            top_indices = np.argsort(hybrid_scores)[-7:][::-1]\n",
    "            top_products = [idx_to_product[idx] for idx in top_indices]\n",
    "            pred = ' '.join(top_products)\n",
    "        else:\n",
    "            pred = baseline_pred\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ed3b1",
   "metadata": {},
   "source": [
    "### What it does:\n",
    "\n",
    "Original Matrix (Sparse, Huge):\n",
    "Users    | Product1 | Product2 | ... | Product24\n",
    "---------|----------|----------|-----|----------\n",
    "User1    |    1     |    0     | ... |    1\n",
    "User2    |    0     |    1     | ... |    0\n",
    "...      |   ...    |   ...    | ... |   ...\n",
    "User931K |    1     |    1     | ... |    0\n",
    "\n",
    "After Factorization:\n",
    "\n",
    "User_Factors (Dense, Smaller):        \n",
    "Users    | F1  | F2  | ... | F15      \n",
    "---------|-----|-----|-----|-----     \n",
    "User1    | 0.2 | 0.8 | ... | 0.1      \n",
    "User2    | 0.9 | 0.1 | ... | 0.7      \n",
    "\n",
    "Product_Factors:\n",
    "Products | F1  | F2  | ... | F15\n",
    "---------|-----|-----|-----|-----\n",
    "Prod1    | 0.5 | 0.1 | ... | 0.3\n",
    "Prod2    | 0.2 | 0.9 | ... | 0.4\n",
    "\n",
    "To predict if User1 should get Product2:\n",
    "\n",
    "Score = (0.2×0.2) + (0.8×0.9) + ... + (0.1×0.4) = 0.85\n",
    "\n",
    "**The core idea:** Instead of saying \"people who bought A also bought B\", Matrix Factorization says \"people who like savings products and digital banking tend to choose these specific products.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b5683ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SVD matrix factorization model with 15 factors...\n",
      "Matrix dimensions: 944901 users x 24 products\n",
      "Sparsity: 0.0646\n",
      "User factors shape: (944901, 15)\n",
      "Product factors shape: (24, 15)\n",
      "\n",
      "Validating MF model on eval set...\n",
      "Generating MF predictions...\n",
      "Processed 100000/931453 users\n",
      "Processed 200000/931453 users\n",
      "Processed 300000/931453 users\n",
      "Processed 400000/931453 users\n",
      "Processed 500000/931453 users\n",
      "Processed 600000/931453 users\n",
      "Processed 700000/931453 users\n",
      "Processed 800000/931453 users\n",
      "Processed 900000/931453 users\n",
      "Users with factors: 926663\n",
      "Users without factors (using baseline): 4790\n",
      "Pure MF MAP@7: 0.3783\n",
      "vs Baseline: -36.27%\n"
     ]
    }
   ],
   "source": [
    "# 1. Build MF model\n",
    "mf_model_svd = create_matrix_factorization_model(\n",
    "    train_df[train_df['fecha_dato'] < '2016-05-01'],  # Train only on data before May\n",
    "    product_cols,\n",
    "    method='svd',  # or try 'nmf'\n",
    "    n_factors=15  # Start with 15 factors\n",
    ")\n",
    "\n",
    "# 2. Test pure MF\n",
    "print(\"\\nValidating MF model on eval set...\")\n",
    "mf_predictions = generate_mf_predictions(\n",
    "    eval_users,\n",
    "    train_df,\n",
    "    product_cols,\n",
    "    mf_model_svd,\n",
    "    top_7_products,\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "mf_map = mapk(eval_truth, mf_predictions)\n",
    "print(f\"Pure MF MAP@7: {mf_map:.4f}\")\n",
    "print(f\"vs Baseline: {((mf_map - 0.5937) / 0.5937 * 100):+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test predictions with alpha=0.05\n",
    "chosen_alpha = 0.05  # Minimal CF influence\n",
    "\n",
    "print(f\"\\nGenerating test predictions with alpha={chosen_alpha}\")\n",
    "test_predictions = generate_hybrid_predictions_fixed(\n",
    "    test_users,\n",
    "    df_train,\n",
    "    product_cols,\n",
    "    item_similarity_df,\n",
    "    top_7_products,\n",
    "    alpha=chosen_alpha,\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ncodpers': test_users,\n",
    "    'added_products': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('hybrid_alpha005.csv', index=False)\n",
    "print(\"Saved to hybrid_alpha005.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c7347c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MF hybrid with alpha=0.05\n",
      "Generating hybrid MF predictions (alpha=0.05: 5% MF, 95% popularity)\n",
      "  MAP@7=0.5927 (-0.17% vs baseline)\n",
      "\n",
      "Testing MF hybrid with alpha=0.1\n",
      "Generating hybrid MF predictions (alpha=0.1: 10% MF, 90% popularity)\n",
      "  MAP@7=0.5910 (-0.45% vs baseline)\n",
      "\n",
      "Testing MF hybrid with alpha=0.15\n",
      "Generating hybrid MF predictions (alpha=0.15: 15% MF, 85% popularity)\n",
      "  MAP@7=0.5884 (-0.89% vs baseline)\n",
      "\n",
      "Testing MF hybrid with alpha=0.2\n",
      "Generating hybrid MF predictions (alpha=0.2: 20% MF, 80% popularity)\n",
      "  MAP@7=0.5878 (-1.00% vs baseline)\n",
      "\n",
      "Testing MF hybrid with alpha=0.3\n",
      "Generating hybrid MF predictions (alpha=0.3: 30% MF, 70% popularity)\n",
      "  MAP@7=0.5877 (-1.01% vs baseline)\n",
      "\n",
      "==================================================\n",
      "BEST MF Hybrid: alpha=0.05, MAP@7=0.5927\n",
      "Improvement over baseline: -0.17%\n"
     ]
    }
   ],
   "source": [
    "# Test different alpha values for hybrid MF + Popularity\n",
    "alphas_to_test = [0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "results_mf_hybrid = {}\n",
    "\n",
    "for alpha in alphas_to_test:\n",
    "    print(f\"\\nTesting MF hybrid with alpha={alpha}\")\n",
    "    hybrid_mf_preds = hybrid_mf_popularity(\n",
    "        eval_users,\n",
    "        train_df,\n",
    "        product_cols,\n",
    "        mf_model_svd,\n",
    "        top_7_products,\n",
    "        alpha=alpha,\n",
    "        reference_date='2016-04-28'\n",
    "    )\n",
    "    \n",
    "    score = mapk(eval_truth, hybrid_mf_preds)\n",
    "    results_mf_hybrid[alpha] = score\n",
    "    print(f\"  MAP@7={score:.4f} ({((score - 0.5937) / 0.5937 * 100):+.2f}% vs baseline)\")\n",
    "\n",
    "# Find best configuration\n",
    "best_alpha_mf = max(results_mf_hybrid, key=results_mf_hybrid.get)\n",
    "best_score_mf = results_mf_hybrid[best_alpha_mf]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BEST MF Hybrid: alpha={best_alpha_mf}, MAP@7={best_score_mf:.4f}\")\n",
    "print(f\"Improvement over baseline: {((best_score_mf - 0.5937) / 0.5937 * 100):+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6a78e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test predictions with MF hybrid model...\n",
      "\n",
      "1. Rebuilding MF model on full training data...\n",
      "Building SVD matrix factorization model with 15 factors...\n",
      "Matrix dimensions: 949614 users x 24 products\n",
      "Sparsity: 0.0649\n",
      "User factors shape: (949614, 15)\n",
      "Product factors shape: (24, 15)\n",
      "\n",
      "2. Generating hybrid predictions with alpha=0.05\n",
      "Generating hybrid MF predictions (alpha=0.05: 5% MF, 95% popularity)\n",
      "\n",
      "3. Saved to mf_hybrid_alpha05.csv\n",
      "Shape: (929615, 2)\n",
      "Unique products recommended: 12\n",
      "\n",
      "First 3 predictions:\n",
      "User 15889: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "User 1170544: ind_cco_fin_ult1 ind_recibo_ult1 ind_ctop_fin_ult1...\n",
      "User 1170545: ind_cco_fin_ult1 ind_ctop_fin_ult1 ind_recibo_ult1...\n",
      "\n",
      "Submit mf_hybrid_alpha05.csv to Kaggle to see if MF adds any value on test set\n"
     ]
    }
   ],
   "source": [
    "# Generate test predictions with MF hybrid\n",
    "print(\"Generating test predictions with MF hybrid model...\")\n",
    "\n",
    "# First, rebuild the MF model on ALL training data (not just pre-May)\n",
    "print(\"\\n1. Rebuilding MF model on full training data...\")\n",
    "mf_model_full = create_matrix_factorization_model(\n",
    "    df_train,  # Use ALL training data now\n",
    "    product_cols,\n",
    "    method='svd',\n",
    "    n_factors=15\n",
    ")\n",
    "\n",
    "# Load test data if needed\n",
    "if 'test_df' not in locals():\n",
    "    test_df = pd.read_csv('/Users/dennis_m_jose/Documents/GitHub/Santander-RecSys-Dennis/data/test_ver2.csv')\n",
    "    test_users = test_df['ncodpers'].values\n",
    "    print(f\"Loaded {len(test_users)} test users\")\n",
    "\n",
    "# Generate predictions with best alpha from validation (0.05)\n",
    "best_alpha_mf = 0.05\n",
    "print(f\"\\n2. Generating hybrid predictions with alpha={best_alpha_mf}\")\n",
    "\n",
    "test_predictions_mf = hybrid_mf_popularity(\n",
    "    test_users,\n",
    "    df_train,\n",
    "    product_cols,\n",
    "    mf_model_full,\n",
    "    top_7_products,\n",
    "    alpha=best_alpha_mf,\n",
    "    reference_date='2016-04-28'\n",
    ")\n",
    "\n",
    "# Create submission\n",
    "submission_mf = pd.DataFrame({\n",
    "    'ncodpers': test_users,\n",
    "    'added_products': test_predictions_mf\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "filename = f'mf_hybrid_alpha{int(best_alpha_mf*100):02d}.csv'\n",
    "submission_mf.to_csv(filename, index=False)\n",
    "print(f\"\\n3. Saved to {filename}\")\n",
    "print(f\"Shape: {submission_mf.shape}\")\n",
    "\n",
    "# Quick stats\n",
    "unique_products = set(' '.join(test_predictions_mf).split())\n",
    "print(f\"Unique products recommended: {len(unique_products)}\")\n",
    "print(f\"\\nFirst 3 predictions:\")\n",
    "for i in range(3):\n",
    "    print(f\"User {test_users[i]}: {test_predictions_mf[i][:50]}...\")\n",
    "\n",
    "print(f\"\\nSubmit {filename} to Kaggle to see if MF adds any value on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed8e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
